{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kishen/.virtualenvs/coqa/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"stanfordnlp/coqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'story', 'questions', 'answers'],\n",
       "        num_rows: 7199\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source', 'story', 'questions', 'answers'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia\n",
      "The Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula. \n",
      "\n",
      "The Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail. \n",
      "\n",
      "In March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online. \n",
      "\n",
      "The Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items. \n",
      "\n",
      "Scholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican. \n",
      "\n",
      "The Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.\n",
      "['When was the Vat formally opened?', 'what is the library for?', 'for what subjects?', 'and?', 'what was started in 2014?', 'how do scholars divide the library?', 'how many?', 'what is the official name of the Vat?', 'where is it?', 'how many printed books does it contain?', 'when were the Secret Archives moved from the rest of the library?', 'how many items are in this secret collection?', 'Can anyone use this library?', 'what must be requested to view?', 'what must be requested in person or by mail?', 'of what books?', 'What is the Vat the library of?', 'How many books survived the Pre Lateran period?', 'what is the point of the project started in 2014?', 'what will this allow?']\n",
      "{'input_text': ['It was formally established in 1475', 'research', 'history, and law', 'philosophy, science and theology', 'a  project', 'into periods', 'five', 'The Vatican Apostolic Library', 'in Vatican City', '1.1 million', 'at the beginning of the 17th century;', '150,000', 'anyone who can document their qualifications and research needs.', 'unknown', 'Photocopies', 'only books published between 1801 and 1990', 'the Holy See', 'a handful of volumes', 'digitising manuscripts', 'them to be viewed online.'], 'answer_start': [151, 454, 457, 457, 769, 1048, 1048, 4, 94, 328, 917, 915, 546, -1, 643, 644, 78, 1192, 785, 868], 'answer_end': [179, 494, 511, 545, 879, 1127, 1128, 94, 150, 412, 1009, 1046, 643, -1, 764, 724, 125, 1384, 881, 910]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['source'][0])\n",
    "print(dataset['train']['story'][0])\n",
    "print(dataset['train']['questions'][0])\n",
    "print(dataset['train']['answers'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kishen/.virtualenvs/coqa/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = dataset['train']['story'][0]\n",
    "question = dataset['train']['questions'][0][0]\n",
    "\n",
    "encodings = tokenizer.encode_plus(question, context)\n",
    "\n",
    "inputs = encodings['input_ids']\n",
    "sentence_embeddings = encodings['token_type_ids']\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run exampl thru model\n",
    "start_scores, end_scores = model(input_ids = torch.tensor([inputs]), token_type_ids = torch.tensor([sentence_embeddings]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m start_i \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m end_i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(end_scores)\n\u001b[1;32m      3\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens[start_i:end_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "start_i = torch.argmax(start_scores)\n",
    "end_i = torch.argmax(end_scores)\n",
    "answer = ' '.join(tokens[start_i:end_i + 1])\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start_i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstart_i\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'start_i' is not defined"
     ]
    }
   ],
   "source": [
    "print(start_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
